*** test 1: sequential vs openmp vs cuda ***

attention_sequential
config=(1,8,8): 0.000021
config=(8,32,32): 0.002246
config=(16,64,64): 0.032248
config=(32,128,128): 0.495715
config=(64,256,256): 7.645053
config=(128,512,512): 120.536278

attention_openmp
config=(1,8,8): 0.002366
config=(8,32,32): 0.003670
config=(16,64,64): 0.006847
config=(32,128,128): 0.038406
config=(64,256,256): 0.426367
config=(128,512,512): 6.179160

attention_cuda
config=(1,8,8): 0.001255
config=(8,32,32): 0.000493
config=(16,64,64): 0.001047
config=(32,128,128): 0.004168
config=(64,256,256): 0.026388
config=(128,512,512): 0.204117

*** test 2: simple vs transposed vs tiling (thread) vs tiling (block) vs flash vs flash multi ***

attention_cuda
-m
t
config=(1,8,8): 0.000409
config=(8,32,32): 0.000538
config=(16,64,64): 0.001172
config=(32,128,128): 0.005189
config=(64,256,256): 0.034289
config=(128,512,512): 0.262377

attention_cuda_tile
config=(1,8,8): 0.001389
config=(8,32,32): 0.000583
config=(16,64,64): 0.001094
config=(32,128,128): 0.005036
config=(64,256,256): 0.037491
config=(128,512,512): 0.364607

attention_cuda_tile
-m
r
config=(1,8,8): 0.000487
config=(8,32,32): 0.000589
config=(16,64,64): 0.001063
config=(32,128,128): 0.007174
config=(64,256,256): 0.030964
config=(128,512,512): 0.254737

attention_flash
config=(1,8,8): 0.001283
config=(8,32,32): 0.000460
config=(16,64,64): 0.002450
config=(32,128,128): 0.018363
config=(64,256,256): 0.329671
config=(128,512,512): 8.476523

attention_flash
-m
m
config=(1,8,8): 0.000356
config=(8,32,32): 0.000620
config=(16,64,64): 0.002853
config=(32,128,128): 0.009248
config=(64,256,256): 0.020914
config=(128,512,512): 2.829161

*** test 3 (unified memory): tiling (block) vs flash vs flash multi ***

attention_cuda_tile
-m
r
-u
config=(1,8,8): 0.000450
config=(8,32,32): 0.000466
config=(16,64,64): 0.000462
config=(32,128,128): 0.000484
config=(64,256,256): 0.000618
config=(128,512,512): 0.001210

attention_flash
-u
config=(1,8,8): 0.000241
config=(8,32,32): 0.000458
config=(16,64,64): 0.002451
config=(32,128,128): 0.018370
config=(64,256,256): 0.329889
config=(128,512,512): 8.480431

attention_flash
-m
m
-u
config=(1,8,8): 0.000274
config=(8,32,32): 0.000272
config=(16,64,64): 0.000280
config=(32,128,128): 0.000287
config=(64,256,256): 0.000296
config=(128,512,512): 0.000292

*** test 4 (tile size): tiling (thread) vs tiling (block) ***

b=128 s=512 e=512

attention_cuda_tile
tile=1: 24.073385
tile=4: 0.670529
tile=8: 0.361450
tile=16: 0.581860
tile=32: 1.473706

attention_cuda_tile
-m
r
tile=1: 12.095971
tile=4: 0.411871
tile=8: 0.255104
tile=16: 0.363575
tile=32: 0.803587

*** test 5 (separate operations) using "tiling (block)" ***

attention_cuda_tile -m r

config=(1,8,8): 0.084397 0.000070 0.000401 0.000013
config=(8,32,32): 0.080492 0.000160 0.000422 0.000022
config=(16,64,64): 0.080012 0.000550 0.000419 0.000098
config=(32,128,128): 0.079734 0.003422 0.000415 0.003300
config=(64,256,256): 0.079848 0.022406 0.000424 0.007988
config=(128,512,512): 0.086689 0.173406 0.000417 0.091301
